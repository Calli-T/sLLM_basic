61p
트랜스포머 아키텍처는 빠르고 성능이 뛰어나 여러 모델에서 다 쓴다
토큰: 자연어 처리 연산의 기본 단위, 단어보다 짧을 수 있다

RNN 성능이 구린 이유 2개
이전 토큰의 출력을 다시 모델의 입력으로 사용
-> 병렬처리 불가
-> 오래전의 정보 희석됨
추가로 층을 깊이 쌓으면 기울기 소실, 증폭 문제 발생
-> 셀프 어텐션으로 해결
그래서 효율성, 긴 프롬포트 사용가능, 효율성 3개의 장점이 생김

셀프 어텐션은 문장 내의 단어 끼리의 연관성을 계산하여 단어의 표현을 조정
-> 아마 셀프 어텐션한 다음 벡터로 만든다는 소리인듯

인코더
[(토큰임베딩 -> 위치임베딩) -> {(정규화 -> 멀티헤드 셀프 어텐션)} 반복 두번, 잔차연결 사용] -> FFN -> 정규화 -> 잔차연결 -> 정규화 디코더의 크로스 어텐션
디코더
[(토큰임베딩 -> 위치임베딩) -> {(정규화 -> 마스크드 멀티헤드 어텐션) 잔차연결 사용} -> (정규화한 값(Q) + 인코더 값(K, V)) -> 크로스 어텐션 -> 정규화 -> FFN -> 정규화] ->
마지막 헤드
-> 정규화-> FC -> Softmax

63p
(Transformer architecture가) 텍스트를 숫자로 변환하는 과정은
잘라서 토큰화 -> 토큰 임베딩 통과-> 위치 임베딩 값 원소 덧셈

토큰화: 적절히 텍스트를 나눠 숫자 아이디를 부여, dict구조
단위는 자모, 음절, 단어 등등
OOV: Out of Vocabululary, dict에 없으면 생기는 문제이다
큰 단위는 OOV가 자주터짐, 작은 단위는 텍스트의 의미가 유지가 안됨
결론: 요새는 등장 빈도에 따라 토큰화 단위를 결정하는 '서브워드 토큰화'를 사용
서브워드 토큰화: 자주 나올 수록 단위를 크게 유지
65p의 예제는 생략

토큰 임베딩
토큰 임베딩은 (dict수_행 x 임베딩의 차원수_열)인 레이어를 통과하면 되며,
해당 레이어가 토큰의 의미를 담기 위해서는 학습이 필요하다

위치 인코딩
모든 입력을 동시 처리하는 트랜스포머에 소실된 순서 정보를 임베딩에 넣어주기 위함
원본은 Sinusoidal함수를 사용한 방식이며,
위치에 따른 임베딩 층을 추가해(이거 분명 DDPM에서 본 것 같은데) 학습 데이터를 통해 학습하는 방식도 있음
절대적 위치 인코딩은 (추론 시점에) 고정된 임베딩을 더해주는 방식, 수식이나 임베딩 층 모두가 해당한다
그러나 학습 때 못본 데이터에 취약하고/토큰과 토큰 사이 상대적 정보를 이용한 위치를 사용하지 못하는 문제가 있어
'상대적 위치 인코딩' (8.4.2절) 방식을 사용한다
근황: 상대적 위치 인코딩을 쓴다 요새는
69p예제 코드는 nn.Embedding(단어수, 차원수)

텍스트를 두 임베딩에 넣은 결과물들을 elementwise하게 더하면 입력 임베딩