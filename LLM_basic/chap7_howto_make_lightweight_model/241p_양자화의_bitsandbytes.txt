{   서론
    요새 모델이 커져서 fp32쓰던거 fp16을 많이들 쓴다고한다.
    8, 4, 3비트로 양자화 하는게 있고,
    4비트로 양자화 한 다음 16비트로 계산하는 'W4A16'가 주류라고한다.

    학습 후 양자화(PTQ)와 양자화 인지 학습(QAT)가 있다.
    둘은 양자화 하는건 같은데 학습 과정에서 양자화를 생각하느냐 마느냐.
    양자화 인지 학습은 학습 과정에서 양자화를 모방하여 모델을 훈련하여,
    정확도 손실이 적으나 더 많은 연산(당연히 양자화 이후를 생각하면선 학습하니까...) 학습 시간이 필요하지만,
    양자화 하고 나서 성능이 뛰어나다.

    bitsandbytes / GPTQ / AWQ는 모두 학습 후 양자화 방식이다
}
비츠앤바이츠
{
    {   4비트 양자화
        4bit 양자화는 책의 5.5장에서 나온 방식, QLoRA 논문의 저자가 만든 방식이다.
        NF4는 64개의 가중치를 묶어 정규화를 진행한 다음
        평균과 표준 편차를 기억해두고,
        해당 값을 0부터 15까지의 4bit정수로표기하고,
        사용할 때는 평균과 + (패러미터가 나타내는 표준편차 배율) * 표준편차 방식을 사용한다
    }
    {   8비트 양자화
        영점 양자화나 절대값 양자화가 있으나,
        성능을 위해 아주 큰 outlier의 열은 별도 분리해서 16비트 그대로 계산한다고 한다.
        정상 범위의 열은 양자화 할 때는,
        벡터 단위(입력의 행, 모델의 열이라고 하는데, 정확하게 텐서의 어느 축인지는 모르겠음)로 한다고 한다.
        입력의 행, 모델의 열에서 최대값이 가장 큰 값을 찾아 양자화 상수로 쓴다고 한다
        정상 값 벡터 끼리, 이상치 벡터 끼리 각각 곱해서 최종적으로 FP16으로 계산한다고 한다.
        -> ??? 뭔소리냐 ??? 그래서 어떻게 동작하는건데? 이상치를 뺀 행렬에서 곱을 했는데 왜 결과물의 크기가 같은건가?
        ->-> 일단 그림에서는 이상치를 빼버렸을 때, 대응하는 행을 통째로 빼버리고 거기 대응하는 열을 통째로 빼버리긴 하더라,
    }

    사용 방식은 load_in_4bit/bnb_config_4bit, bnb_4bit_quant_type='nf4',
    8비트의 경우 숫자만 앞의 두 옵션은 이름이 숫자 4->8이며, quant_type은 따로 없다
}