multi head attention
여러 어텐션 연산을 동시에 적용하여 성능 올리기
여러가지 '관점'을 추가하는 기술

멀티 헤드 어텐션의 구현은
(관례적으로 내부 벡터 길이와 동일한) 임베딩 길이를 헤드의 수로 나누
여기에 대응하는 선형층도 [S, d] 에서 [S, d // num_head]를 num_head개수로 바꾼다
원본 QKV와 대응하는 헤드의 선형층을 사용하여 어텐션하면 [S, d//num_head]의 행렬이 num_head개 나오고
이를 병합하면 [S, d]행렬이 나오며,
N개의 배치에 대해 수행했을 경우[N, S, d]행렬이 최종적으로 나온다
(책에서는 concat_layer로 nn.Linear(d_inner, d_inner)를 만들어 한 번 더 통과시켜준다)

여기까지 요약
구현: 원래 어텐션의 선형 레이어를 헤드 수만큼 만들어주고, 차원을 [S, d_inner//num_head]로한다
그러면 어텐션 결과가 [S, d // num_head]인게 헤드 수만큼 나오므로 이를 병합하면 멀티헤드 어텐션이다.
의미: 여러 측면에서 이해하는 장치

82p
층 정규화: (한 데이터의)의 한 채널에서 같은 공간적 위치의 모든 값을 (주로 N(0, 1)으로) 정규화 하는 것
층 정규화 하는 이유는 범위가 서로 다른 특성간의 분포를 동일하게 맞춰 영향력을 동등하게 하는것

나이는 대략 [1, 100], 키는 [140, 200]사이의 값을 가지기 때문에 정규화 안하면 영향력이 다르게 반영된다.
norm_x = (x-평균) / 표준편차
저 식으로 계산하면 ~N(0, 1)

자연어 처리는 프롬프트 길이가 서로 다르기 때문에 배치 정규화가 어렵다.
길이가 짧은 시퀀스와 정규화 하다보면 PAD토큰의 값과 정규화해야하는데, 그럴 수는 없지않은가

층 정규화는 토큰 임베딩 각각에 (즉 [N, S, d]의 d에 대해) 수행한다
그렇게 되면 모든 토큰의 스케일이 N(0, 1)를 따르게 된다

84p
정규화는 어텐션 이전에 하는 것이 안정적이라고 한다
이를 사전 정규화라 한다
(생각해보면 당연한 문제이기는 하다)
(일단 영향력을 동등하게 만들어 놔야 가중치가 안정적으로 동작할 것 아닌가)
(같은 영향력을 가져야, 어떤 판단에 가중치를 높일지를 고를 수 있다)
그리고 LayerNorm(embedding_dim)을 쓰면 간단하게 구현가능
요약: 정규화는 어텐션 이전에 함 + LayerNorm 함수로 구현 간단히 가능
85p
피드 포워드 층: FC랑 드롭아웃, 활성화함수, 정규화 등등으로 만든 그거,
정규화 끝난걸 넣어서 패턴을 분석하도록 만든다
입력 텍스트 전체를 이해하도록 만든다고 한다?
관계에 관한 값을 넣어 패턴을 분석하는 신경망이니 그럴듯

86p
잔차연결: 출력에 쌩 입력만 원소합으로 더한거, 깊이 쌓는데 도움을줌
이 모든것을 N회 반복하면 그게 인코더다

87p88p에는 앞선 코드를 n번 반복하는 트랜스포머 인코더 예제 코드에 대해 나와있다
