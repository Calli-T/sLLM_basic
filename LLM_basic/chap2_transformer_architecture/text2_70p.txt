70p
어텐션은 입력한 텍스트 중 어떤 단어가 서로 관련되는지 '주의'를 기울이는 방식이다
문장에서 앞뒤 단어로 '맥락'을 파악하여 단어를 읽을 수 있는것이 사람의 글읽기 방식이고, 이를 모방한 것이 어텐션이다
관련이 깊거나 얕은 단어를 맥락에 반영할 수 있어야한다
정확히는 Query에 대해 유사한 Key를 찾고 대응하는 Value를 가져오는 것이다

72p는 그 방식이다
QKV는 각각 검색어(또는 토큰의 임베딩), 검색어와 대조할 문서(혹은 데이터)의 제목이 키, 문서의 내용이 값

쿼리와 키의 임베딩 행렬을 선형 변환 하는 이유
쿼리와 키를 토큰 임베딩으로 변환하면 이를 내적하여 맥락이 계산 가능하고
문자열 일치가 안되더라도 유사한 키를 꺼낼 수 있다
그러나 한 단어로 하면
1) 같은 단어끼리는 너무 높은 연관도가 나오거나,
2) 별 관련 없는 단어 끼리의 간접적인 연관성을 반영하지 못하는 경우가 있어
가중치로 이를 해결하며, 이것은 Q와 K에 임베딩 행렬에 곱해지는 행렬을 의미한다.
W_Q와 W_K는 각각 다른 행렬이며 학습됨

여기까지 나온거
※ 임베딩 차원과 트랜스포머 내부 차원이 동일한것은 관례이다
토큰화 -> 토큰/위치 임베딩 원소합으로 만든 최종 임베딩 -> Q,K,V 각각 다른 선형층(=행렬)에 통과시킴 -> QKV텐서 획득

어텐션 메커니즘
Q,K,V 각각 다른 선형층(=행렬)에 통과시킴 -> QKV텐서 획득 ->
QK 행렬을 서로 곱함 -> 내부 차원의 제곱 근으로 값을 나눔 -> softmax를 취해 (합이 1인) 가중치를 구함 -> 이를 밸류 행렬과 곱함

※ sqrt(d_inner)로 값을 나누는 이유는 출력의 분산을 제어하기 위함, 자세한건 나중에
※※ 이를 요약하면 Attention(Q, K, V) = Softmax(QK^T / sqrt(d))V
※※※ 내부 텐서가 [N, S, d]차원인데, 쿼리와 키는 d가 서로 같아야 S*S행렬을 만들어낸다
※※※※ 이를 [S, d_inner]인 V행렬과 곱하여 최종적으로 [S, d]텐서, 즉 Q와V행렬의 d는 내부 차원과 같을 필요가 없고, V의 d는 d_inner와 같아야한다
※※※※※ Softmax에 넣는것은 V의 seq의 각 차원 얼마나 집중되는지를 나타낸 값이다
※※※※※※ 최종 형태는 [N, S, d], 이는 각각 배치, 토큰 길이, 내부 차원의 크기이다.