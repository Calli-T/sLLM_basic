189p
PEFT(Parameter Efficient Fine-Tuning): 일부 파라미터만 학습시키는 방법들의 통칭
LoRA(Low-Rank Adaptation): 모델에 일부 파라미터를 추가하고 그 부분만 학습하는 방식, PEET방법중 하나
QLoRA: 모델 파라미터를 양자화하는 QLoRA

{ LoRA 상세:
    LoRA는 모델 파라미터를 재구성하여 더 적은 파라미터를 학습함으로 GPU사용량을 줄인다.

    LoRA의 파라미터 재구성은 행렬을 더 작은 2개의 행렬의 곱으로 표현해,
    전체 파라미터를 수정하는 것이 아니라 더 작은 2개의 수정하는 것을 의미함

    예시 이미지는 190p 부터
    100x100차원 레이어가 있다면,
    100 x r차원 행렬과 r x 100행렬 두개를 준비하고->
    이를 행렬곱한다음 ->
    원본 행렬(의 가중치는 고정, LoRA 행렬은 학습)의 값과 더하고 ->
    순전파하고 역전파 방식으로 학습한다

    전체 미세 조정에 비하여, 학습할 파라미터 수의 8% 이하, 1% 미만의 파라미터 등 훨씬 적게 사용한다

    Q: 모델 전체 파라미터는 들어있는데, 대체 왜 VRAM 사용량이 감소하는가?
    A: 원본 행렬의 가중치가 고정이므로 그레이디언트와 옵티마이저 상태가 없다!
    -> 모델 전체 파라미터 + LoRA파라미터 + LoRA 그레이디언트 + LoRA 옵티마이저가 VRAM에 들어가고,
    LoRA는 모델 원본보다 훨씬 적기 때문!

    LoRA의 설정은 192p부터
    1. rank값 r의 크기 ※ 이 rank라는게, 선형대수학에서 말하는 선형 독립인 행(열)의 수인 Rank로 추측된다
    행렬곱으로 원래 차원 dxd행렬로 돌아갈테니, d x r, r x d행렬 두개의 행렬을 준비한다.
    이 때 r의 크기는 비용과 성능의 trade-off, 실험적으로 알아봐야하는 값
    2. 더해질 때 LoRA의 반영 계수 α,
    LoRA에서는 (알파/r)만큼의 비중으로 기존 파라미터 W에 더해준다
    -> 알파가 커질수록 새롭게 학습한 파라미터의 중요성을 크게 고려
    또한 실험적으로 알아가야하는 값
    3. 모델의 어떤 파라미터를 재구성 할 것인지 결정
    셀프 어텐션의 AKV 가중치나, 피드포워드 층 등 일부나 전체에 적용가능
    -> 일반적으로 전체 선형층에 LoRA를 적용한 경우가 가장 성능이 좋은 것으로 알려져있으며
    실험을 통해 알아가야함

    코드는 193p
    추후에 한 번 돌려보자
}

195p
{   LoRA에 양자화를 추가해 메모리 효율성을 한 번 더 높인 학습 방법
    {   분포가 주어진 데이터를 쉽게 양자화하는법
        데이터를 순서를 사용하는 양자화는 데이터를 정렬하고 순서를 기억해야하므로,
        연산과 메모리 사용량이 증가하는 문제가 있다
        -> 그렇다면 기존 데이터의 분포를 알고 있다면 이를 개선할 수 있는가?
        -> 가능하다

        196p에는 그 예시가
        만약 데이터가 정규 분포를 따른다면,
        이를 기반으로 균등하게 구간을 N등분으로 나눌 수 있다
        이를 구현한 함수 quauntile_normal도 예시로 나와있는데,
        정규 분포 그래프 아래 면적의 특정p%가 몇 이하에 존재하는지를 표시해준다
        0.5를 입력했을 때 0이 나오는 예시는 전체 데이터 50%가 0보다 작다는 소리이다
        0.6을 입력했을 때 0.2533이 나오는 예시는 전체 데이터 60%가 0.2533보다 작다는 소리이다
        이 값의 차이를 통해 10%의 데이터가 (0, 0.2533)의 범위에 있다는 사실을 알아낼 수 있다.
        이를 16등분하면 값을 0~15에 매핑 가능
    }

    학습된 모델 '파라미터'는 거의 정규 분포에 가깝다고 알려져있다.
    -> 입력은 대체로 정규분포라고 가정해도 상관없음!!!
    -> 빠른 양자화 가능

    {  2차 양자화
        NF4 양자화 과정에서 64개의 모델 파라미터를 한 블럭으로 묶고 (GPT에 따르면 채널 단위로 가능),
        평균은 0으로 고정하고 표준편차(32비트짜리)를 블럭마다 저장하는 것이다.

        이 32비트짜리 표준편차 상수 256개를 다시 하나의 블록으로 묶어 8비트 양자화를 시키면, 상수 저장에 필요한 메모리도 저장할 수 있다.
        이전에 32비트 데이터 256개를 저장하다가 8비트 데이터 256개와 양자화 상수 c를 저장하기위한 32비트 데이터 1개만 저장하면된다
    }

    + 4비트 양자화에 쓰는 LUT 테이블은 구간을 비선형적으로, 0에 가까울 수록 촘촘하게 나눠놨다.
    ++ 가장 가까운 값에 대응하는 비트를 주는 방식이며, 해당 테이블은
    LUT={−1.0,−0.784,−0.588,−0.431,−0.302,−0.194,−0.102,−0.018,0.018,0.102,0.194,0.302,0.431,0.588,0.784,1.0}

    198p
    {   페이지 옵티마이저
        사용이유: 그레이디언트 체크포인팅을 사용하면, 지점에 따라 메모리 사용량이 갑자기 튀는 경우가 존재한다
        이 때 OOM 오류가 나올 수 있다.
        이런 순간에 대응하기 위해 사용하는 것이 OOM

        개념: 엔비디아의 통합 메모리를 통해 CPU 메모리(RAM)을 공유하는 개념
        컴퓨터의 가상 메모리 시스템과 유사하다
        RAM이 모자랄 때 디스크를 끌어다 쓰는것처럼,
        VRAM이 모자랄 때 RAM을 끌아더 쓴다.

        메모리의 페이지/세그먼트 할 때 그 페이지 개념이 맞는듯함, 유사하게 동작
        VRAM이 꽉차면 이를 RAM에 저장한다.
    }

    199p 부터는 코드 예제가!
}