459p
{   멀티 모달 LLM
    멀티 모달 LLM: 텍스트, 이미지, 비디아, 오디오, 3D 등 다양한 형식의 데이터를 이해하고 생성할 수 있는 LLM
    ※ 멀티 모달: 여러 유형의 데이터를 함께 처리하는 AI 기술

    2024년 기준 텍스트와 이미지가 가장 활발히 현구중이고
    멀티 모달 생성 보다는 멀티 모달 이해 성능을 높이기 위한 기술 개발에 집중

    # 기본 개념 둘
    {   멀티 모달 LLM의 구성요소 ※ 이미지-텍스트 모델 위주의 설명
        1. 모달리티 인코더: 이미지를 의미 있는 표현으로 변환(인코딩) ※ ViT, CNN 파생 모델 등 사용가능
        2. 입력 프로젝터: 그 인코딩 등을 LLM 입력 토큰(의 시퀀스)으로 정렬
            ※1 ViT의 patch 하나 당 하나의 (token) embedding 공간으로 사상, linear layer(입력은 patch를 flatten 한걸 받아옴), MLP 등 사용
            ※2 텍스트는 원본 llm 방식 그대로 tokenizer -> embedding 하면되고, 결과물을 이미지 토큰 임베딩과 concat하(거나 cross-attention 등등을 하)면된다.
        3. LLM (백본)
        4. 출력 프로젝터: 이미지 형태의 출력이 필요한지 판단한다.
        5. 모달리티 생성기: 특정 데이터 형식의 출력을 생성한다.

        이중 모달리티 인코더, LLM, 모달리티 생성기는 사전 학습된걸 그대로 사용하는 경우가 많다.
        입력 프로젝터와 출력 프로젝터는 LLM이랑 같이 학습을 해야한다!

        이미지와 텍스트를 같은 벡터 공간안에 사상시켜야 하므로,
        그런 걸로 학습한 전용 모델인 CLIP(CLIP-ViT 등)을 쓰면된다.
    }

    {   멀티 모델 LLM 학습 과정
        백본으로 사전 학습된 텍스트 LLM을 쓰면된다.

        멀티모달 입출력을 지원하기 위한 추가 훈련 단계를 거치는데,
        LLM 학습 때와 마찬가지로 사전 학습과 SFT(지도 미세 조정)로 나뉜다.
        사전 학습 때 LLM은 이미지-텍스트 쌍과 같은 대규모 멀티 모달 데이터셋으로 학습된다.
        그런 다음 소규모 멀티 모달 지시 데이터셋으로 미세 조정한다.
        ※ 멀티 모달 지시 데이터의 예시로는
            1) 이미지 캡션 생성
            2) 입력 이미지에 대한 질문 응답
        멀티 모달 지시 데이터셋은 특정 멀티모달 작업을 수행하도록 학습하는 것이다.
        SFT 당시에 쓴 데이터셋과 유사한 데이터에 탁월한 성능을 발휘한다고 한다.

        입/출력 프로젝터만 학습한다고 책에 쓰여있는데
        (GPT-4o 피셜) 모든 파라미터를 학습하는 것이 일반적이다. 대신 경우 모델들의 원본 파라미터가 변형될 수 있음.
        (Gemini 2.5 Pro 피셜) 입출력 프로젝터만 학습하는 것이 일반적이며, 모든 파라미터도 학습 가능하고 성능도 좋으나 비용문제를 고려해야한다
        (Copilot 피셜)  입출력 프로젝터만 학습하는 것은 매우 효율적이나. 전체를 학습 할 수도 있다.
        세 AI가 말하는 내용을 쭉 보면, 선택의 폭이 꽤 있는 모양이다.
        책에는 입출력 프로젝터는 전체 파라미터의 1~2% 정도라고 하니, 효율적이긴 할 것이다.
    }

    #
    {   CLIP 모델
        CLIP은 텍스트 데이터와 이미지 데이터의 관계를 계산할 수 있도록
        텍스트 모델과 이미지 모델을 함께 학습시킨 모델이다

        # 데이터 준비
        학습에는 서로 관련이 있는 이미지-텍스트 쌍을 사용한다.
        ※ 464p의 예시는 MS-COCO 데이터로, 코끼리를 관람하는 사람의 사진과, 동물원에서 사람들이 코끼리를 구경하고 있다는 캡션이 한 쌍으로 되어있다
        CLIP 모델의 학습에는 50만개의 검색어로 4억 개의 쌍을 구축했다고 한다.

        # 학습 방법
        대조 학습을 사용한다. ※ InfoNCE Loss를 사용한다고 한다 !!! 이 부분은 나중에 한 번 정리 들어가자.
        텍스트와 이미지 양 임베딩 모델이 있고,
        서로 매칭되는 데이터 쌍은 임베딩 했을 때 (임베딩 벡터 거리가) 가까운 방식으로 동작하도록 학습한다고 한다.
        반대로, 매칭이 안되는 데이터 쌍은 멀어지도록 학습한다.

        텍스트 임베딩 모델은 Transformer 계열, 이미지 인코더는 ViT나 CNN 파생 계열을 쓴다.
    }

    {   CLIP 모델의 강점
        제로샷 추론이 된다. ※ 사전 학습 이후 레이블이 있는 데이터로 미세 조정 하지 않았을 때의 추론을 말한다.

        A photo of a {객체}를 여러 단어를 돌려가며 임베딩으로 넣고,
        이미지를 임베딩 벡터로 바꾸면,
        여러 텍스트 임베딩 중 서로 유사도가 가장 높은 것에 대한 이미지가 된다... 고 한다

        텍스트를 넣어 해당하는 이미지를 검색할 수도 있다.

        성능은 벤치마크 데이터셋에 따라 다른데, 위성 사진이나 의학 사진 등 전문 도메인의 데이터는 잘 동작하지 않는다고 한다.
    }
}