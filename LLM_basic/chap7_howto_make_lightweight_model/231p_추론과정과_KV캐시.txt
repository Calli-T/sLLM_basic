231p
다 아는 설명 그거
확률적으로 어떤 토큰이 몇 %인지 정해서 제일 높은 그거
소프트맥스 그거

생성을 종료하는 법은
EOS같은 특수 토큰 생성하거나, 최대 길이에 도달한 경우

232p
여태까지 생성한 토큰은 죄다 병렬 처리가능, 그래서 토큰 1개 생성하는 시간은 각각 비슷함

따라서, 추론 과정을 프롬프트를 처리하는 단계인 사전 계산 단계와,
한 토큰씩 생성하는 디코딩 단계로 구분한다고 한다.

디코딩 단계에서, 셀프 어텐션은 필연적으로 여러번 반복적으로 계산하도록 되어있다.
첫 단어는 첫 단어와 어텐션, 두 번째 단어는 첫 단어와 두 번째 단어와 어탠션, 세 번째는 첫 번째, 두 번째, 세 번째와 어텐션...
먼저 나온 단어일 수록 반복을 많이 한다.

KV 캐시에는 Key / Value 텐서를 저장해둔다
{   KV캐시에는 무엇을 왜 저장하는가?
    (그 텐서라는게 아마 원래 임베딩을 key행렬과, Value행렬에 집어넣은 값일 것이다)
    (Query는, 현재 생성 중인 토큰에 따라 매번 갱신되므로 캐시에 넣는 것이 비효율적이라고한다)
    자세한 설명은 이쪽에
    https://jins-sw.tistory.com/entry/LLM-Inference%EB%A5%BC-%EB%B9%A0%EB%A5%B4%EA%B2%8C-%ED%95%98%EC%9E%90-GQA-SWA-KV-Cache-Flash-Attention-Speculative-Decoding
    뭔 소린가 했는데, 프롬프트 -> 임베딩 -> 선형변환을 거쳐서 처음 만든 K와 V는, 디코딩 과정 내내 고정이라고 한다
    이는 ChatGPT, Gemini, Copilot이 다 맞다고 얘기하더라
}
※ decoder only 모델은 어텐션을 블럭 당 한 번 한다고 한다.

234p
KV 캐시의 크기는 다음과 같다고 한다
2byte(fp16은 2byte) x 2(K&V) x 레이어 수(모든 레이어가 토큰 하나 생성에 한 번 어텐션) x 토큰 임베딩 차원 x 최대 시퀀스 길이 x 배치 크기

llama-2 13B 모델 기준, 모델 자체는 26 GiB고 KV는 배치당 3.125 GiB라고 한다.