331p
텍스트를 임베딩 벡터로 바꾸는걸 텍스트 임베딩 혹은 문장 임베딩이라 부른다

{   빠르게 넘기죠
    일단 문장 임베딩의 필요: 자연어를 컴퓨터가 처리 가능한 상태로 만들어야함
    ※ 벡터 연산으로 유사도를 구할 수 있음 ※※ 코사인 유사도? 그러나 문장은 행렬로 나와서, 풀링해야함
}

{   332p ~ 임베딩 이전의 방식
    원핫: 하나만 1인거
    백오브 워즈: 단어빈도수 집계, 그러나 많이 나오는 단어가 (조사같은거) 꼭 의미 파악에 도움이 되진 않음
    TF-IDF: 백오브워즈의 단점 개선, 많은 문서에 나오면 쓸모 없도록 처리함
    한 단어에 w대해, TF(등장한 횟수), 와 DF(등장한 문서수)로 두고, 문서수 N으로 두고
    TF(w) * log(N/DF(w))로 계산함
    문서수가 많으면 빈도가 커도 의미를 파악하는데 딱히 쓸모없음
    그러나 차원 수가
    워드투벡:함께 등장하는 빈도의 정보를 사용해서 의미 압축, 이후 문장 임베딩으로 이어짐
    주변 단어로 가운데 단어를 알아보는 CBOW, 가운데 단어로 주변을 예측하는 스킵그램이 있으나
    둘 다 고전 기법이 됨

    요약: 결국 밀집 임베딩 개념만 남기고 다른 방식으로 넘어감
    희소 벡터란? 단어를 몇 개의 차원을 가진 벡터에 의미를 압축해서 요약하는것

    ※ 요새는 트랜스포머 기반 사전학습된 모델을 쓴다고 한다
}

337p
여기서 개념 정리 하나,
문장 임베딩을 만드는 모델은 엄밀히 따지면 tokenizer와는 다르다
RAG용 벡터는 '한 줄짜리' 벡터이며
llm 트랜스포머에 적용하는 모델은 토큰 당 한 줄, 즉 시퀀스 길이 만큼의 행을 가진 벡터이다. (열은 당연히 차원)
이 장에서 말하는 임베딩은 행렬이 아니라 벡터에 가깝더라
※ transformer의 tokenizer에서 쓰는건 BPE가 유명함

{   문장 임베딩 방식
    {   SBERT와 그 구현체
        문장 임베딩 라이브러리 중 유명한 것은 sentence-transformer이고, 이는
        SBERT의 구현체이며, 이는 BERT의 응용이다

        {   왜 BERT가 백본으로 채택되었나
            BERT는 원래 양방향 transformer 인코더 구조를 가진 encoder-only transformer 모델이다
            문장 내 MASK된 단어 예측과, 두 문장이 이어지는지를 사전 학습했다.
            그 둘 중 단어 하나하나를 주변 문맥에 맞춰 인코딩하는 능력이 핵심이고, 사전학습되어있으므로 SBERT의 재료가 되었다
            요약: 단어를 문맥에 맞춰 이해하고 문장을 인코딩할 수 있는 (사전학습된) 모델이 BERT였다.
        }

        {   어떻게 BERT로 SBERT를 제작하는가?
            {   문장 사이의 관계를 계산하는 두 가지 방법과 효율
                바이 인코더: 두 문장을 각각 BERT의 입력으로 넣고, 모델은 임베딩을 출력, ※ 코사인 유사도 등으로 임베딩 벡터간 거리 계산
                크로스 인코더: 두 문장을 함께 BERT의 입력으로 넣고, 모델이 직접 두 문장 사이의 거리를 (0, 1)로 직접 출력한다

                효율성을 위해 SBERT는 바이 인코더 방식을 사용한다
                크로스 인코더는 1개의 문장을 1000개의 문장과 찾으려면 BERT에 찾는 문장과 대상 문장의 쌍을 1000번 집어 넣어야한다
                반대로, 바이 인코더는 미리 임베딩으로 변환된 1000개의 문장에서 코사인 유사도만 1000번 수행하면된다
                BERT모델은 어텐션을 쓰므로, 시간, 연산 부하가 상당하다
                찾을 때마다 BERT모델을 쓰느니 임베딩만 저장해서 코사인 유사도를 구하면 된다.

                요약: 문장 검색에는 임베딩만 저장해서 코사인 유사도 하는 것이 최고, 매번 임베딩 모델을 쓰기에는 오버헤드가 상당함
                바이는 한 문장을 넣어 임베딩 출력을, 크로스는 두 문장을 넣어 유사도 출력을 낸다.
                ※ 요즘 벡터 DB는 코사인 유사도 같은 연산을 DB 내부에서 처리할 수 있도록 최적화 되어 있다고 한다.
                ※※ Sentence-Transformer는 바이 인코더를 지원한다고 한다.

                더 짧게 요약: 문장을 하나씩 임베딩으로 만들어서 DB에 저장해두고, 코사인 유사도로 검색해라
            }

            {   바이 인코더의 모델 구조
                1. 문장을 BERT에 넣는다.
                2. 토큰별로 나뉜 출력 임베딩을 풀링 하여 하나의 임베딩으로 요약한다.

                343p
                풀링은 CLS 토큰'만' 쓰는 방식, 특수 토큰을 제외한 평균 값을 쓰는 방식, 최대 값을 쓰는 방식이 있는데
                ※ https://velog.io/@dutch-tulip/Sentence-BERT
                Mean풀링을 주로 쓴다고 한다. 특수 토큰을 제외한 값의 평균을 사용
            }
            ※ LLM의 tokenizer와 RAG용 tokenizer가 반드시 일치할 필요는 없다. 사실 안하는게 일반적
            ※※ SBERT의 tokenizer는 당연히 BERT의 것을 따라감
        }
    }

    요약: 문장째로 BERT에 넣어서 토큰 별 임베딩 -> 풀링해서 문장의 임베딩 -> 이걸 DB에 저장 -> 벡터간 거리로 검색
}