BERT는 bidirectional encoder representations from transformers,
인코더 모델이며
토큰의 일부를 마스킹한 다음 이를 맞추는 방식으로 사전 학습한다
학습 다하면 다운 태스크 작업에 따라 미세조정한다

인코더 모델은
자연어 이해를 잘하며
입력에 병렬 연산이 되므로 학습과 추론이 빠르고
다운스트림 성능이 뛰어남-(당장 HuBERT음성 모델만해도...)

생성은 빡세고
컨텍스트 길이는 제한적임

HuBERT는 토큰에 텍스트 대신 음성을 사용하는 것이고,
마스크도 음성 신호를 마스킹하는 것이며,
클러스터링을 사용한다

원리는 나중에

BERT는 토큰 분류, 자연어 추론 등 자연어 이해 작업에 훌륭한 성능을 보임

-----

GPT3.5는
175B모델, 96 레이어, 모델의 차원 12288, 토큰 2048개이다

-----

97p
인과적 언어 모델링: gpt등등에서 쓴다. 이전 토큰들을 보고 앞의 단어를 처리함
마스크 모델 모델링: bert 등에서 쓴다. 일부 토큰을 마스크 하고 처리