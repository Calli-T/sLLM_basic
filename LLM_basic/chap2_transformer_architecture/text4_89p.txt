디코더는 생성을 담당하는 부분이다.
한 블록에서
첫 어텐션으로 마스크드 셀프 어텐션을 쓰고,
두 번째 어텐션에서 크로스 어텐션을 쓴다.

어텐션 마스크란?
(오리지널) 트랜스포머 아키텍쳐의 디코더는 앞단어부터 순차적으로 작성하고
이를 casual, auto-regressive라 한다.
이를 구현하기 위해 만든 것이 마스크드 어텐션이다.

어텐션 마스크의 적용 시점?
softmax를 취했을 때 영향이 아예 없도록한다.
소프트맥스 이전의 QK^T 즉, 어텐션 스코어에 곱하여 적용하는 마스크이다

작동 방식은?
어텐션 마스크는 참고해서는 안될 영역에 -INF를 둔다
어텐션 스코어의 i행 j열의 값은, 시퀀스의 i번째 토큰과 j번째 토큰의 연관성을 나타낸 것이다.
따라서, 이를 -INF로 둔 다는 것은 아예 연관이 없다는 뜻이다.
이를 위한 마스크 행렬은 상삼각 행렬과 비슷한 형태의 위치가 -INF의 값이다.
그 외에는 그대로 둔다는 의미의 1
1행에는 한 값이, 2행에는 두 값이...

※ 대체로 디코더를 한 번 통과시키면, 토큰이 하나 생기는 구조이다
그리고 디코더는 디코더 블럭과 이후의 정규화 -> 선형층 -> 소프트맥스를 전부 포함한다.
※※ 디코더의 입력은 곧 출력 시퀀스이며 이를 반복해서 넣어 길게 만든다.
※※※ 단, 추론과 훈련의 동작이 다르다.
훈련 때는 입력 데이터셋의 페어로 있는 출력 데이터셋의
전부를 입력하여 마스크로 현재 시점 이전의 토큰들을 attention하여 생성하고
결과물을 데이터셋의 현재 시점의 토큰과 비교하여 loss를 계산한다.
추론 때는 시작토큰 이외에는 전부 패딩이며 이를 하나씩 출력 시퀀스로 바꿔나가는 것이다.

디코더 블럭의 첫 어텐션은 입력을 출력시퀀스로 넣고, 마스크드 셀프 어텐션을 진행한다

91p
디코더의 두 번째 어텐션은 크로스 어텐션을 진행한다

요약: 크로스 어텐션의 방법은 일반적인 어텐션과 같으며
Q는 디코더의 마스크드 셀프 어텐션의 출력에서
KV는 인코더의 출력에서 나오는 텐서이다

요약: 크로스 어텐션의 의미는
(현재까지의)출력시퀀스와 입력시퀀스 간의 연관성을 계산하여, 대응하는 Value
즉 다음 토큰을 만들어 낼 수 있도록(또는 그렇게 될 텐서를 만드는) 하는 작업이다.

※※※※
대부분의 LLM은 decoder - only 구조이다
이유는?
https://introduce-ai.tistory.com/entry/%EC%99%9C-%EB%8C%80%EB%B6%80%EB%B6%84%EC%9D%98-LLM%EC%9D%80-Decoder-only-%ED%98%95%ED%83%9C%EB%A1%9C-%EA%B5%AC%EC%84%B1%EB%90%98%EB%8A%94%EA%B1%B8%EA%B9%8C

1. 디코더의 기본적인 기능 즉,
자기 회귀적, 대규모 비지도 학습(딱히 정답은 없음)으로도 강력한 제로샷 성능을 발휘 할 수 있기 때문
대규모로 corpus를 때려박으면 뭐 어떻게든 돌아가기는 하는 모양

용어설명
자기회귀는 스스로의 출력을 다시 변수로 쓴다는 뜻, 여기서는 이전 출력 토큰을 현재 토큰 생성에서 어텐션으로 쓴다는 얘기인듯
딱히 클래스가 없으므로 비지도 학습임
제로샷인건 배운적 없는 클래스, 프롬프트에 대해서도 학습이 가능함

2. 창발성
모델의 사이즈가 임계점을 학습 능력이 크게 상승한다고 함
이는 decoder-only 모델에서 주로 발견됨
최소 1B 모델이상, sLLM이상 크기에서 발견됨
학습한 정보끼리 상호 작용을 할 수 있는 폭이 넓어져서?
모델이 크면 담을 수 있는 패턴도 많아지고,
패턴을 둘 이상 동시에 사용할 수 있는 작업도 가능해지게 되기 때문에?
자전거 예시는) 균형 잡기와 폐달을 동시에 해야하는데, 이 둘을 동시에 배울 수 있다면 비로소 자전거도 탈 수 있는것이 아닌가?


?????
3. ICL관련 수학적인 능력
출처 : Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers

4. 구조적으로 자기회귀적 학습에서 K, V 재사용 가능 -> 속도와 계산에 대한 큰 이점
5. 마찬가지로 구조적인 이점, masked attention에서 full-rank효과???? 하삼각 형태는 causal???