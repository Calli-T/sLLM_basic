LLM을 서빙하는 비용이 GPU에 영향을 받음
같은 GPU로 처리량을 높이고 지연 시간을 낮추면 효율적으로 서빙 가능

{   GPU 상식
    GPU는 연산을 수행하는 부분과 / 계산할 값을 저장하는 SRAM(=L1 캐시)
    SRAM는 작기 때문에 HBM(= 고대역폭 메모리)에 큰 값을 저장

    A100의 specification
    108개의 Streaming Multiprocessor
    에 각각 192KB SRAM, 합쳐 20MB를 좀 넘음
    HBM 40GB, 대역폭 1,555GB/s
}

{   최적의 배치 구하는 법
    모델 파라미터의 메모리가 P이고, fp16을 사용할 때, 계산량은 2 * P * (배치크기) 바이트
    계산을 위해서는 HBM에서 SRAM으로 이동해야며,
    이 과정에서 P 만큼의 메모리를 옮기는데 시간이 필요하다(배치는 일단 모델보다 적으니 고려 안하는 것인가???)

    연산 시간 = 2*P*배치크기/하드웨어 연산속도(FLOPS)과
    모델 파라미터 이동 시간 = P/메모리대역폭 이 값이 서로 같도록 해줘야한다
    최적의 배치 크기보다 커지거나 작으면 다른 한 쪽이 멈추면서 비효율이 발생
    위 식을 정리하면
    배치크기 = FLOPS / (메모리대역폭 * 2)

}
230p의 라마 예시는 최적이 5였는데, 모델의 크기와 KV 캐시의 크기를 줄이면,
더 많은 배치를 올릴 수 있게된다.

239p
{   KV 캐시 메모리 줄이기
    {   그룹 쿼리 어텐션
        문제점: 멀티헤드 어텐션은 다양한 측면을 저장, 반영하여 성능을 높이지만, KV 캐시를 위시한 메모리 오버헤드가 상당하다.
            모든 쿼리가 하나의 키와 값 벡터를 공유하는 '멀티 쿼리 어텐션' 방식으로 줄일 수 있다.
            그러나 더 적은 키와 값 벡터를 사용하므로 성능이 떨어지는 문제가 있음
        절충: 멀티헤드보다는 키와 값을 수를 줄이지만(원래는 헤드 개수만큼 있다!),
            멀티쿼리보다는 많은 키와 값 벡터(정확히는 근원 임베딩과 가중치 층을 곱한 값)를 사용하는 것이다.
            그룹 끼리는 같은 키와 값 벡터를 가지게 된다.
        장점: 추론 속도 향상, KV 캐시 메모리 감소
            왜 쓰는가?: 멀티 헤드 어텐션 만큼 빠르면서, 성능은 미세하게 안 좋은 정도임
            기존 학습 데이터의 10% 정도만 사용해도 성능이 멀티 헤드 어텐션에 가깝게 회복된다

        llama-2과 llama-3도 그룹 쿼리 어텐션을 잘 썼다고 한다.
    }
}