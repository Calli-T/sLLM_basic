439p
{   LLMOps
    {   LLMOps는 MLOps랑 뭐가 다른가
        일단 대부분의 머신러닝 모델보다 크다.
        그리고 상업용 모델을 IT 공룡들이 API 형태로 제공함, 오픈소스도 많음 ※ 그리고 학습을 처음부터 시작하는 경우가 적음
        일단 하는건 생성 작업이라 정량 평가가 어려움 ※ 분류나 회귀에 못쓰는 건 아님
    }

    {   모델을 어디서 가져올 것인가?
        머신러닝은 분류나 회귀등 하나의 문제를 해결하는 모델이나,
        LLM은 모델의 파라미터의 수가 많고 여러 문제를 해결하기 위해 만들어지는 경우가 많다. ※ 다운태스크건, 여러 문제를 물어보건, 사전학습된걸 다른데 쓰건간에...
        그래서 많은 패턴을 담을 수 있어야하고, 모델이 커지는 추세이며, 리소스도 많이 먹기 시작했다.

        그리고 큰 모델이라 비용이 많이 드는데다가,
        해결하고자 하는 문제와 난이도도 서로 다름에 따라
        상업용 모델과 오픈소스 모델로 나뉘게 되었다.

        상업용은 보통 쉬운 난이도에 고성능이나 / 버전 변경에 따라 프롬프트가 취약하며 / ※ 미세 조정은 회사 정책 별로 다름
        오픈 소스는 미세 조정이 자유롭고 / 대신 직접 인프라를 관리해야한다

        오픈 소스 모델을 쓰려면
        1~7장의 양자화같은 추론 경량화, lora나 플레시어텐션 같은 학습 효율 상승의 모든 기술을 다 꼴아박아야함
    }

    {   문제는 어떻게 해결할 것인가?
        LLM을 사용례에 맞게 최적화하려면
        사전 학습 / 미세 조정 / 프롬프트 엔지니어링 / RAG
        오픈 소스 모델은 모든 것이 자유롭지만 ※ 사전 학습은 그래도 리소스가 너무 많이 들어서 빡세다
        상업 모델은 미세 조정이 불가능하거나 제한되고 프롬프트 엔지니어링와 RAG가 가능하다

        ※ 사전 학습의 경우
        보통 머신러닝은 처음 부터 직접 모델을 학습했으나, LLM은 힘들다 ※ 파라미터를 세는 단위가 10억이다
        70B 모델은 사전 학습에 20만 달러가 들어간다

        ※ 미세 조정의 경우
        지도 미세 조정이나 DPO같은 경우는 사전 학습보단 비용이 덜하지만, 여전히 모든 파라미터를 쓰므로 많은 GPU가 필요하다
        LoRA나 QLoRA를 쓰면 성능과 비용 trade-off 어느정도 가능

        ※ 프롬프트 엔지니어링의 경우
        학습이 완료된 LLM의 경우, 프롬프트의 변경으로 성능 향상을 어느정도 끌어올릴 수 있다
        템플릿 / 메개변수 / 컨텍스트 관리 / 단계적 프롬프트 설계 등등
        MLOps에서 하이퍼파라미터나 데이터셋에 따라 실험과 버전 관리를 했는데,
        이 경우,프롬프트도 실험 대상이 된다.
        W&B나 MLflow등에서 사용가능

        ※ RAG의 경우
        검색할 문서와 임베딩 모델과 벡터 DB를 만들어 파이프라인을 만들면,
        프롬프트에 RAG 결과물을 반영하여 보강할 수 있다.
        위에 셋을 추가적으로 관리하고 운영해야함
    }

    {   LLM의 평가는 어떻게 할건가
        {   LLM은 평가가 왜 어렵나?
            생성물에 대한 정량 평가가 어렵다. ML은 정확도, 재현율, F1 점수 등 정량 지표가 있음
            작업도 다양하고 답도 다양한 갈래가 있어서 특정 지표로 모두 평가할 수는 없음
            애시당초에 완전히 풀리지 않은 문제임
        }

        {   지금 쓰는 지표의 종류는?
            {   정량적 지표의 경우
                BLEU: 기계 번역 결과와 사람이 번역한 결과의 유사도를 측정
                ※ n-gram 기반으로 (사람이 번역한) 참조 문장과 모델이 생성한 문장 간의 정밀도 측정
                ※※ 문장의 유창성이나 문법적 오류는 반영 불가
                ※※※ 사람의 번역이 필요하고, 여러 개의 번역문이 있으면 더 나음

                ROUGE(Recall-Oriented Understudy for Gisting Evaluation): 모델의 요약문과 사람이 작성한 요약문을 사이의 n-gram 중복도를 재현율 관점에서 측정함
                ※ gisting: 특정 문서나 대화의 요점을 요약하는 과정
                ※※ 단어의 순서나 문장 구조는 고려하지 않음
                ※※※ n-gram recall은 모델이 생성한 n-gram의 일부가 실제 문장에서 얼마나 잘 일치하는지를 측정
                ※※※※ n-gram은 n개 연속된 단어의 묶음

                perflexity: 모델이 새로운 단어를 생성할 때의 불확실성을 수치화한 것
                ※1 크로스 엔트로피의 지수화된 버전, 2^H(p)라고 한다
                ※2 H(p)는 엔트로피를 의미하며, 수식은 H(p) = -Σ(i=1, n, p(x_i) * log(p(x_i)))
                ※3 정보이론에서 사건하나의 정보량은 −log(p(x_i))이나, 확률을 곱해야하므로 p(x_i)를 곱하는 것
                ※4 위 수식에서 전체 불확실도의 합을 계산할 수 있게되는 것이다.
                ※5 정보량은 확률이 높으면 적고 낮으면 높다. 혼란도는 해당 사건이 발생했을 때 제공하는 정보량으로 정의한다.
                ※6 값이 2^H(p)로 나왔다면, (평균적으로) 선택지 중에 하나를 선택해야하는 상황이라고 해석할 수 있다.
            }

            {   벤치마크 데이터셋을 활용한 평가
                정량 지표는 데이터셋에 따라 달라지기 때문에, 여러 모델을 동일 선상에서 평가하기 위해서는
                벤치마크 데이터셋을 사용한다

                ARC, HellaSwag, MMLU, TruthfulQA 등등이 있음
                ※ 한국어는 KLUE, KorQuAD, KoNLPy 등등...

                ARC는 질문에 대한 ABCD 선택 4지선다형.
                HellaSwag는 이어질 문장 4지선다.
                MMLU는 과학 분야 4지선다
                TruthfulQA는 신뢰할 수 있는 모델인지에 관한? 4지 선다※ 예시 문장은 유사 과학을 부정하는 질문

                lm-evaluation-harness 깃허브는 모델명과 평가에 사용할 작업을 이름으로 넣으면 평가 결과 확인 가능
                ※ 직접 해주는건지, 벤치 내용 로그를 찍어주는지는 모름

                한국정보화진흥원과 업스테이지에서는 한국어 LLM 리더보드가 있음
                ※ 업데이트 중단에 가까움
                ※ ChatGPT 추천픽은 Open Ko-LLM Leaderboard
            }

            {   사람이 직접 평가하는 방식
                정량적 지표는 모델의 빠른 평가가 가능하나, 사람의 평가와 일치하지 않는 경우도 많다
                그래서 사람이 직접 LLM의 생성 결과를 보고 확인하는 방법으로 보완해야한다.
                ※ A/B 테스트라는게 있는데, 프롬프트, 모델, 파라미터 등등 서로 (주로 하나만?)다르게 하고 성능을 실험해 보는 것이다.
                근데 이건 시간이 많이 걸린다.
            }

            {   LLM으로 평가
                벤치마크 데이터셋은 얼마나 똑똑한지는 평가가 되는데,
                사람의 요청에 잘 응답하는지는 판단이 불가능하다고 함
                그리고 그건 사람이 평가는 할 수 있는데,
                다른 LLM이 하는게 시간과 비용이 많이 절약된다.

                ※ MT-bench의 멀 티턴 데이터와, 챗봇 아레나 데이터셋을 사용한 실험에서, 사람의 평가와 LLM의 평가가 80% 일치 했다고 한다.
                ※ 이건 LLM 성능 따라 꽤 갈릴듯

                {   멀티 턴 질문 데이터셋이란?
                    대화형 인공지능 모델의 성능을 평가하고 훈련하기 위해 설계된 데이터셋으로,
                    여러 차례의 상호작용을 포함하는 질문과 응답의 시퀀스를 제공
                    맥락을 반영하는 연속적인 상호작용을 통해 모델이 응답하도록 만듬
                }
                {   챗봇 아레나란?
                    동일한 요청에 대해 2개 모델의 대답을 보여주고
                    더 선호되는 쪽을 고른 일종의 DPO 데이터셋
                }
            }
        }
    }

    {   번외: RAG는 어떻게 평가할 것인가?
        평가는 검색 단계와, 검색한 내용을 바탕으로 생성하는 단계가 있다고 한다.
        ※ 검색해온 문서를 바로 넣을 수도 있지만, 보통은 그걸 활용해서 새로운 응답을 생성해서 넣는다고 한다.

        {   평가할 내용
            맥락 관련성: 검색 결과이 잘됐나? (= 사용자 프롬프트와 맥락 데이터가 관련이 높나?)
            신뢰성: RAG 생성 결과가 검색 결과를 잘 반영했나?
            답변 관련성: RAG 생성 결과가 사용자 프롬프트랑 관련이 높나?

            검색 결과와 생성 결과는 서로 일관된 동시에 사용자와 프롬프트와 연관이 높아야함
        }

        LLM 평가에 쓰는 지표와 방식을 변형하면 대부분 RAG에도 사용가능하다고 한다.
        ※ Ragas 프레임워크는 평가에 LLM을 쓴다고한다. 여기에는 Q & A & context & ground truth가 필요하다고 한다.
    }
}

452p
1. 데이터 준비 -> 학습 -> 배포 -> 모니터링 -> 반복을 자동화하는것이 MLOps
2. LLM로 넘어오면 비용 문제로 대부분 사전 학습된 모델을 미세 조정하거나
상업 모델은 미세 조정도 제한적이라 프롬프트 엔지니어링 / RAG 등을 쓴다고 함 ※ 오픈소스라고 해서 저 두 개가 안되는건 아님
3. LLM은 태스크가 여러 개고, 답도 여러 개에 고정된게 없다. 정확한 평가 방법이 아예 확립이 안됐으므로, 정량지표 / LLM 평가 / 사람 평가 / 벤치마크 데이터셋 등등 다양한 방법으로 평가함
4. LAG는 평가 방법은 LLM과 비슷하고, 검색 결과와 생성 결과는 서로 일관됐는지, 동시에 사용자 프롬프트와 연관이 높은지를 평가한다.