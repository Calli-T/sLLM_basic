174p
{ GPU 사용량과 모델 타입 체크 예제
사용하는 polyglot-ko-1.3b 모델과 토크나이저 정보에 관해서는
https://littlefoxdiary.tistory.com/109 참조
1.3B 모델은
아까 계산한 대로, 추론은 2.37 GiB / 학습은 9.49 GiB VRAM사용함
}

175p
{   그레이디언트와 옵티마이저 상태의 메로리 사용량을 계산하는 함수 예제 코드
    함수가 따로 있는것은 아니고 각각의 파라미터에서 기울기 항을 뽑아내거나,
    옵티마이저를 인자로 받아 보내는 것으로 계산하는 함수를 아예 예제로 박아놨음
}
잡팁1: 기울기 체크포인트 기능이 있음, 기본 값 False
잡팁2: 기울기 누적 기능, 기본값은 1임
값이 N이면 N회 가중치를누적하여 업데이트 한다
(기울기 누적이란??
배치 크기가 크면 학습이 어려우므로,
작은 배치를 여러번 돌려서 누적된 가중치를 업데이트 한 방식 )

{   딱히 안중요함
    176p에는 학습하면서 vram 체크하는 코드를 예시로 써놨다
    그리고 그 아래 make_dummy_dataset은 모델 테스트용 무작위 값 생성 코드
}

177p
이건 중요한거
VRAM(과 캐시) 비우기
del 모델 -> gc.collect() -> torch.cuda.empty_cache()

{   위 내용 복합사용하는 예시 코드, 딱히 안중요함
    178p
    위 내용을 종합적으로 사용해서
    (학습하면서 vram 사용처별 사용량 체크하고
    데이터셋 생성
    학습
    학습 후 vram과 캐시 비우기)
    179p
    이걸 배치 크기를 늘려가면서 학습
}

164p부터 179p까지는
'내 GPU VRAM은 왜 터지나'에 대한 해답,
OOM은 참 가슴아픈일