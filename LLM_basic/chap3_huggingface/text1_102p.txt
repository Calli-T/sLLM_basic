허깅페이스 라이브러리 활용법이다.
간단히 보고 치우자

잡팁
모델, 데이터셋, space(모델 테스트 장소)가 있다

잡팁
한국어 평가 데이터셋은 klue

109p
잡팁
영어권 LLM 리더보드는 Hugging Face Open LLM Leaderboard에 있고 sLLM의 성능 벤치마크에 좋다

112p
잡팁
허깅페이스는 config.json의 model_type을 보고 동류의 모델을 판단하다고 한다.

로베르타 모델, 토크나이저, 로베르타-감정분류 모델
걍 불러오는 예제인데, 무시하자

추가 헤드가 있는 모델에 기본 모델을 불러오면 분류 헤드 레이어의 가중치는 없다는 경고문이 뜬다고하는데
당연한 소리니 걍 읽어보고 넘어가자

116p
잡팁
transformers/AutoModel, AutoTokenizer 클래스는 from_pretrained에 (허깅페이스에 돌아다니는)모델 이름만 박아도 자동으로 가져온다
from 어쩌고 import 모델명 어쩌고 할 필요가 없음

117p
토크나이저 사용법, 특수 토큰등등 설명하는중
시작토큰, 특수토큰, 패딩마스크 등등...

직접 찾아본 내용
Gemini Sentencepiece, GPT3은 byte level BPE를 토크나이저로 쓴다고 한다

120p
datasets load_dataset('데이터셋 이름', '서브셋(필요하다면)')으로 데이터 가져오는 모습
인자로 train주면 train만 가져오는 것도 가능
근데 이건 뭐 사용법 나와있으니 아무 상관없다

뭐 로컬 csv나 파이션 딕셔너리로 변환, 판다스 데이터 프레임 활용등 잡다한 내용도 있음

121p
간단한 실습이니 해보자
허깅페이스에서 다운받은 모델 지워서 용량 비우느거 잊지말고