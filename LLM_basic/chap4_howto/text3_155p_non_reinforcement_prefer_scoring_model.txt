다음은 사전학습 - 지도 미세 조정이 끝난 모델에
선호되는 답변을 만드는 3번째 학습에 관한 내용이며,
강화 학습을 사용하지 않는 방식이다.

기각 샘플링
155p
{   무시하쇼
    기각 샘플링: 최고점을 받은 데이터를 사용
    지도 미세 조정을 마친 LLM에 여러 응답을 생성하고
    그중 리워드 모델이 가장 높은 점수를 준응답을 모아
    다시 지도 미세 조정을 수행
    리워드 모델이 뭔지, 구현이 어떻게 되어있는지 좀 알아야할 듯???

    뭔가 이것저것 난립하는데
    156p의 그림에
    기각 샘플링 -> PPO에서 두 과정을 RLHF로 지칭하더라, 또 미세 조정 내부에 포함되어있음
    이거 강화학습 안쓰는 방법 아냐?
}

요약: 기각 샘플링은 임계값 이하의 샘플을 기각하는 방식이다.
+ 모델의 출력을 필터링 하거나, 학습 데이터에서 필터링을 하는 두 가지 방식 모두 가능

그 자체는 강화학습이 아니지만 PPO나 RLHF같은 강화학습 방식과 같이 사용하는게 가능함
LLaMA는 기각 샘플링과 RLHF를 모두 쓴 사례, 학습 데이터 필터링 과정에서 기각 샘플링을 사용
필터링 하고 RLHF로 넘어갔음

157p
직접 선호 최적화(Direct Preference Optimization): 강화 학습 없이 선호 데이터셋을 직접 학습하는 것이다
RLHF는 LLM의 '출력에' 더 선호되는 모델을 학습하는 거,
DPO는 선호 데이터셋을 그냥 주는거

출력여러개 -> 보상 계산 -> 보상이 높은 출력을 기준으로 역전파를 진행
GPT피셜)
{
    1. RLHF에서 출력과 보상 학습 과정
    RLHF는 다음과 같은 단계를 거칩니다.

    (1) 모델이 여러 개의 출력을 생성
    모델이 같은 입력(prompt)에 대해 여러 개의 응답을 샘플링합니다.
    예를 들어, 하나의 질문에 대해 3~5개의 다른 출력을 생성할 수 있습니다.

    (2) Reward Model(RM)로 보상 점수 계산
    사람이 각 응답을 비교하여 선호하는 응답을 선택하고, 이를 이용해 **보상 모델(Reward Model, RM)**을 학습합니다.
    Reward Model은 LLM이 출력한 응답에 대해 점수를 부여하는 역할을 합니다.
    이후, 새로운 응답이 들어오면 보상 모델이 평가하여 높은 점수와 낮은 점수를 매깁니다.

    (3) 높은 보상을 받은 출력에 대해 역전파
    Proximal Policy Optimization (PPO) 같은 강화학습 알고리즘을 사용하여,
    높은 보상을 받은 출력을 더 자주 생성하도록 모델의 파라미터를 업데이트합니다.
    즉, 보상이 높은 출력을 생성하는 방향으로 모델이 학습되도록 역전파를 수행합니다.
    반대로, 낮은 점수를 받은 응답은 덜 생성되도록 조정됩니다.
}

158p
DTO는 선호 데이터셋에서 선호 데이터가 비선호 데이터보다 예측 확률이 높아지도록 학습함
참고 모델로 지도 미세 조정을 마친 모델을 사용하여 학습이 잘되고 있는지 확인한다

논문의 부제
'당신의 언어모델은 리워드 모델이기도 하다'

DTO전에 지도 미세 조정은 필수라고 한다
지시사항과 응답의 형식을 이해해야하기 때문

160p
선호 데이터셋을 구축하는 작업이 필요함-당연하게도
AI를 사용한 방법들은?
허깅페이스의 제퍼-7B-베타
제퍼는 4개의 LLM이 생성한 결과를 또다른 AI가 평가하고 가장 높은 점수를 받은 선호 데이터와
나머지 3개중 랜덤으로 하나를 선택한 비선호 데이터 쌍을 수축,
이런방식으로 하는 DPO를 dDPO라 하는데 앞의 d는 지식 증류(knowledge distillation)이다

뉴럴-챗-7B는
GPT3.5나 4로 생성한 답변을 선호 데이터로,
LLaMA 2-13B모델이 생성한 답변을 비선호 데이터로 사용,
단순히 성능이 더 뛰어난 모델의 생성 결과를 선호 데이터로 선택함으로
학습 과정을 단순화

앨런 70B 모델에서도 DPO가 성공함으로 대형 LLM도 가능하다고 어느정도 인정받은 상태

요약: 어느정도 큰 모델에서도 DPO가 동작함을 보인바 있으며
선호 데이터셋을 사람이 만들 필요 조차 없이
다른 LLM의 결과물을 또다른 AI가 평가한 것을 쓰거나,
아니면 LLM의 성능 격차를 이용해서 구축해도 된다

그리고 DPO는 강화 학습이나 리워드 모델 둘 다 필요없어 2024기준 가장 선호됨
DPO의 실제 방법에 대해서 확인해 볼것