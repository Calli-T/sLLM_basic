166p
1개 GPU도 메모리(아마 VRAM)을 효율적으로 활용할 수 잇는 방법
그레이디언트 누적, 그레이디언트 체크포인팅을 소개한다고 한다

분산 학습과 단점과 해결책을 제시

전체 모델을 업데이트하지 않고 모델 일부만 업데이트 하는 LoRA 도 소개

모델을 더 적은 비트를 사용하는 데이터타입으로 저장하여 메모리 효율을 높이는
QLoRA에 대해 알아본다.

167p
요새 fp32대신 fp16이나 bf16을 용량 문제로 사용한다고함

168p
fp32는 지수 8, 가수 32비트
bf16은 지수 8, 가수 7비트
fp16은 지수 5, 가수 10비트

bf16은 fp16에 비해 더 넓은 범위를 덜 정밀하게 표현

169p
양자화: 더 적은 비트로 모델을 표현하는 기술
원본 데이터의 정보를 최대한 유지하면서 더 적은 용량의 데이터 형식으로 변환하려는 것이 핵심

{   좀 더 상세한건 5-5장과 8장에서 나온다고함
    예시에서는 fp32의 min, max를 int8의 min, max로 대응하는 간단한 양자화를 선보였으나,
    데이터가 없는 양 fp32의 양쪽 끝에 대응하는 int8의 공간이 낭비되는 사례를 설명

    170p
    절대값의 최대 최소로 하여 공간이 낭비되는 것을 방지하는 사례를 설명

    그리고 이상치가 있을 때 잘 동작하지 않는 문제를 설명
    outlier가 있을 경우 절대 값이 이상치에 의해 정해져서 대부분의 데이터가 중앙에 있어
    낭비되는 공간이 많음을 역설함

    이상치의 영향을 덜 받는 법은?
    블록 단위의 양자화: 3개씩 데이터를 묶어 그 안에서 그 안에서 절대 최댓 값을 구하고 변환을 수행
    퀀타일 방식: 절대 최댓값말고도 입력 데이터를 크기순으로 등수를 매겨 in8 값에 동일한 개수의 fp32값이 대응되도록 배치하는 방식
    다대일 매칭 방식
    (이건 등수를 확인하고 배치해야해서 계산량이 많고
    순서를 기억해야해서 별도의 메모리도 필요함)
}

172p
GPU에 올라가는 요소 모음
모델 파라미터/그레이디언트/옵티마이저 상태
(+순전파 상태)
파라미터는 싹 다 GPU VRAM에 올라감 - N
기울기는 오차역전파를 위한 요소(손실 함수에 대한 각 파라미터의 편미분) 순전파할 때 기억해놨다가 역전파할 해서 학습함 - N
(adamw는 파라미터 업데이트를 위해 평균과 분산 2개의 모멘텀 항을 저장) 2N
모델 용량이 N이면 VRAM은 4N만큼 필요하다

7B 모델의 용량은 13.11 GiB 정도

잡팁: 허깅 페이스에서 모델 메모리 계산기 지원함
173p
잡팁: vram 사용량은 print_gpu_utilization