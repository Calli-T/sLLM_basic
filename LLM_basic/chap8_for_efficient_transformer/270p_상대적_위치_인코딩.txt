{   상대적 위치 인코딩
    {   나온 배경
        기존에는 고정된 방식으로 삼각함수를 사용해서 위치 인코딩, 즉 토큰에 위치 정보를 주었다
        그러나 학습 데이터보다 더 긴 입력이 나오면, 생성 품질이 떡락했다

        그래서 절대적 위치 말고 토큰끼리 상대적인 위치 정보를 추가하는 '상대적 위치 인코딩' 도입
    }

    {   직관적인 이해 방법
        '검은 고양이가 밥을 먹고 물을 마신다'
        '어제 아침에 길에서 검은 고양이가'
        두 문장에서, 토큰 '검은'과 '고양이' 사이는 한 칸 떨어져 있으나,
        절대적 위치 인코딩에서는 서로 다른 위치 임베딩 값이 더해질 수 밖에 없는 구조
    }

    {   방식의 종류
        RoPE와 ALiBi가 있다
        아키텍쳐 따라 다르며, GPT-4는 RoPE를 쓴다고 한다(Gemini피셜)
        일단 RoPE가 주류라고함

        {   RoPE, Rotary Positional Embedding
            각각의 토큰 임베딩을 토큰 위치에 따라 회전시키는 방식
            '검은 고양이가 밥을 먹고 물을 마신다' <- 이 문장에서
            '검은'은 첫 토큰, '고양이가'는 두 번째 토큰이므로 이를 각각 θ, 2θ
            '어제 아침에 길에서 검은 고양이가' <- 이 문장에서
            같은 토큰은 각각 4θ, 5θ만큼 회전

            이러면 위치 인코딩은 달라지지만, 두 임베딩 사이에서 각도는 동일하다
            위치 정보가 각도를 통해 모델에 반영되는 구조

            '회전'시킨다는게 뭔소린가 했는데,
            토큰 각각에 주어진 만큼 각 토큰의 임베딩 전체를
            아핀변환 '회전' 하는 것이었다. 기하학적인 회전이 맞다
        }

        {   ALiBi
            토큰 임베딩 자체를 변형하는게 아니고, [N, N]크기의 어텐션 스코어 행렬에 선형적인 편향을 더하는 식이다
            현재 쿼리의 위치를 의미하는 0을 기준으로 앞에 있을 수록 더 작은 값을 더해 상대적인 위치를 나타낸다... 라고 설명은 했는데,
            첫 행은 0, 둘 째 행은 [-1, 0], 세번 째 행은 [-2, -1, 0]... 이런식으로
            현재 쿼리의 오른쪽 끝을 기준으로 왼쪽으로 갈수록 더 작은 값을 더해 상대적인 위치를 나타낸다고 볼 수 있다
        }

        책에서는 절대적 위치 인코딩 < RoPe < ALiBi, 순서대로 성능이 좋아진다고 하는데(기준은 펄플렉시티)
        실제론 좀 더 복잡한 모양이다
        ALiBi는 대신 학습과 추론에 들어가는 오버헤드가 적은 모양
    }
}