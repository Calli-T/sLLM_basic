선요약: AWQ는 모델의 활성화 값 분포를 통해 중요한 파라미터를 결정하고, 이를 기반으로 (스케일러)등의 양자화를 수행하는 양자화 기법임


AWQ는 모든 파라미터의 정보가 중요한 것은 아니라는 아이디어에서 출발
중요한 파라미터만 정보를 유지시키면 괜찮을 것,

-> 중요한 파라미터를 찾는 방법은?
1. 큰 값을 지닌 패러미터 고르기
2. 활성화 값이 큰 채널의 패러미터 고르기

-> 그래서 실험해봄
활성화 값의 상위 1%만 남기고 나머지를 FP16으로 썼을 때는 성능 저하 X
모델 파라미터 크기를 기준으로 했을 때는 성능 저하 O

-> 다른 데이터 타입이 섞여 있을 경우 일괄 연산이 어렵다는 문제 발생
FP16말고 모델 파라미터의 정보를 유지하는 방법은?

-> 이를 찾기위해 양자화 했을 때 성능이 떨어지는 이유를 파악
(대충 숫자 뭉그러지는 이유)
-> 그럼 어떻게 할 것인가?
중요한 파라미터 1보다 큰 값을 곱하는 방식으로 이 문제를 해결,
이를 스케일러라고함
250p
펄플렉시티라는 지표로 스케일러 크기에 따른 성능을 평가,
펄플렉시티: 토큰에 다음 단어가 올지 확신하지 못하는 정도, 낮을 수록 좋은 지표이다.
이 지표로 봤을 때 스케일러 s는 2가 적당하다는 사실이 '실험적으로' 발혀졌다.

스케일러가 클 경우 거기 맞춰, 다른 파라미터가 좁은 범위로 변환된다는 얘기를,
그래서 성능이 떨어진다는 얘기를 하는데 사실 이건 좀 더 자세한 구조를 봐야 알 것같다.

251p
라이브러리는 llm-awq이며, 이미 불러온 모델은 autoawq 라이브러리가 필요하다
불러오는 코드가 해당 페이지에 존재함

+ GGUF는 모델 저장 형식인 동시에, 형식 자체에도 양자화 정보를 포함한다.
저 셋 라이브러리와는 퍽 다른 형식이라고 한다.
양자화 방식은 CPU 기반 추론에 최적화 되어 있다고 한다. (GPU도 된다, 단지 GPU가 제한된 장치에서 대규모로 모델을 사용하는것)
이는 GGML이라는 라이브러리에 기반해있으며,
pytorch의 모델을 GGUF 형식으로 변환해서 사용할 수 있다고 한다.