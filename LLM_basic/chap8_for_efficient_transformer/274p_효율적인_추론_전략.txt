배치 전략과 아키텍쳐 말고도 추론에 도움이 될 만한 전략이 존재함
KV 캐시도 그 방법 가운데 하나 ※ KV캐시는 학습에는 사용하지 않음, LoRA같은 경우는 원본 모델이 추론을 하니까 필요한것

페이지 어텐션: KV 캐시가 차지하는 메모리를 효율적으로 관리함
커널 퓨전: GPU에서 반복적으로 수행하는 연산을 하나로 묶어 더 효율적으로 처리함
추측 디코딩: 명확하고 쉬운 단어는 작은 모델이, 어려운 단어는 큰 모델이 처리하는 기법

274p
{   커널 퓨전
    GPU의 연산은 커널 단위로 이뤄진다.
    ※ 그리고 커널이 뭔지 책에 안써놨는데, GPU 내부용 함수다. OS 커널이 아니다!!!
    ※※ 예시로 2개의 텐서를 곱할 때 Matmul 커널 사용함
    ※※※ call이 아니라 launch라고 부른다
    ※※※※ 플래시어텐션 커널 퓨전을 사용함

    연산의 시작 전후에 추가적인 작업을 위한 오버헤드가 발생한다.
    대표적인 오버헤드는 HBM에서 read/연산한거 write
    N개의 커널이 수행되는 경우, 오버헤드도 N배 이벤트를 진행함!@!!
    그래서 반복적으로 수행하는 연산은 커널단위로 분리하는 것보다,
    연산을 묶어서 오버헤드를 줄이는게 낫다고 함
    read/write가 1/N배! 오버헤드도 1/N배!

    플래시어텐션도 커널 퓨전을 사용함
    어텐션 행렬을 한 번 넣은건 필요한 연산?함수?(커널)를 싹 처리해버리고 나오니까, 맞는 말이긴 한듯
}

275p
{   페이지어텐션
    추론과정의 어텐션 연산에서 반복되는 K와 V(W_K와 W_V를 통과한 K와 V 행렬)을 저장해서 연산 시간을 줄인다
    그러나 이건 그대로 VRAM 오버헤드이고 이를 줄이는 방법이 페이지 어텐션

    {   KV 캐시의 낭비가 발생하는 이유
        최대 토큰 수 만큼의 메모리를 예약해뒀으나, EOS가 일찍 나오면 전부 안씀
        이것들은 연속적인 물리 메모리이다
    }

    {   어떻게 낭비를 줄이는가?
        OS의 페이지 스왑과 유사한 방법을 쓴다
        - 논리적 메모리와 물리적 메모리를 연결하는 블록 테이블을 생성
        - KV 캐시가 논리적 메모리에서 연속적으로 할당되도록 할당
        - 한 번에 페이지 크기 만큼만 예약을 한다 -> 그러면 많아야 (페이지 내부 블록 개수 - 1) 만큼만 낭비된다 ※ 페이지의 크기는 GPU 사양 따라, 설정 따라 다르다
        - 페이지 내부의 블록을 다 연산하면 swap_out하고, 새로 넣고 하는 식으로 처리하면 된다
        ※ cpu memory, 그러니?까 램에다가 swap out한다고 한다
    }

    {   응용
        병렬 샘플링: 동일한 프롬프트에서 여러 개의 출력을 생성하는 디코딩 방법
        - 프롬프트를 2개 이상의 논리적 블럭에 넣고,
        - 서로 다른 토큰을 생성 했을 때 분리,
        - 분리할 때 같은 토큰 개수 만큼 '참조 카운트'로 기록,
        참조 카운트: 물리적 블럭을 공유하고 있는 논리적 블럭 수
        - 참조 카운트가 1보다 클 경우 (새로운 토큰을 바로 이어서 저장하지 않고) 새로운 물리적 블럭에 기존 토큰을 복사,
        - 원본의 카운트를 1로 줄임
        - 복사된 토큰은 거기서부터 생성
    }
}

280p
{   추측 디코딩
    문장에 따라 쉽게 다음 단어를 예측할 수 있는 문장과 아닌 문장이 있다.
    언어 모델도 그러할 것이라는 생각에 기반한 방법
    쉬운 단어는 작은 모델이, 어려운 단어는 큰 모델이 처리하도록 하는 방법이다.

    {   단어의 예측 난이도를 판단하는 방법
        요약: 드래프트 모델로 결과를 어림짐작하고 타깃 모델로 확인하는 방식

        작은 draft 모델과 큰 target 모델 2개의 모델을 준비,
        작은 드래프트 모델로 빠르게 K개의 토큰 생성,
        타깃 모델이 추론했을 때의 결과와 동일할지 계산,
        동일하면 승인/동일하지 않다면 거절

        디코더만 있는 언어모델은 기본적으로 프롬프트에 뒤이어 토큰을 덧붙이는 방식이다.
        거기서 K개를 드래프트 모델이 생성하고, 0개부터 K개까지 타깃 모델의 판단과 일치하는 개수를 본다음
        타겟 모델이 승인한 개수만큼 토큰을 추가하고, 나머지는 타깃 모델이 직접 생성
    }

    {   그래서 타켓 모델은 토큰을 어떻게 평가할건데? ※ 책에 없음
        (GPT피셜임)
        입력과 이전 생성 토큰들로 해당 토큰의 등장확률(logits)를 계산하고
        임계값을 걸어 처리를 한다고 한다

        한 줄 평:
        자세한 정보가 없는걸보니 최신기술이긴 한듯

        (GPT 피셜임)
        그럼 로그잇은 어떻게 계산하는데?
        드래프트 모델의 마지막 벡터(최종 은닉 상태)를 선형 변환하고, 소프트 맥스를 걸고, 로그를 취한다
    }

    GPT는 안쓰는 기술이라고 한다(GPT 피셜)
    282p
    구글에서 친칠라 모델(70B)과 어떤 드래프트(4B)모델을 추측 디코딩에 사용하여 생성 시간을 절반 내외로 줄일 수 있었다고 한다
}

283p 부터는 기회될 때 실습해보자