다음은 사전학습 - 지도 미세 조정이 끝난 모델에
선호되는 답변을 만드는 3번째 학습에 관한 내용이며,
강화 학습을 사용하는 방식이다.
{

    예시는 코테 모델에 가독성을 채점하는 모델로 채점하는 방식

    148p
    채점 모델의 데이터셋 구축하는 방법
    가독성이 좋은 코드와 덜 좋은 코드를 두고 더 선호하는 걸 선택한 데이터셋을 선호 데이터셋이라함
    선호를 구하는게 원문을 직접 비교해서 점수를 매기는 것보다는 쉬운 작업인듯

    선호데이터셋을 구축하면, 선호 데이터셋을 비선호 데이터보다 높은 점수를 주도록 채점 모델을 학습

    149p
    사용방볍 예시
    채찍피티를 개발하는 과정에서 사용된 방식

    요약: 지도 미세 조정에서 응답을 다수 생성
    -> 다수 생성한 응답중 레이블러가 더 좋은 답변을 순서대로 정함
    -> 순서가 높을 수록 채점 점수가 높도록 학습

    해당 방식으로 가독성 좋은 코드, 위험한 정보 회피, 선호 대답 등등 만들 수 있음

    그럼 어떻게 선호도 순위별로 점수를 매기는가?
    그 방법은
    150p
    강화학습
    작동 방식 요약
    에이전트가 환경에서 행동을하여
    ->환경의 상태가 바뀌고
    ->행동에 대한 보상이 생기는데,
    ->수행하는 행동의 모음을 에피소드라고 함
    ->이 보상을 크게 하는 방식으로 에피소드가 짜이게됨

    151p의 가독성 좋은 코테 대답 채점 모델에서 강화학습의 요소를 예시로 듬
    에이전트가 코드쓰고, 채점 시스템은 환경, 채점 시스템이 매긴 점수는 보상, 행동으로 변화된 코드는 상태, 높은 점수를 받기위한 여러 행동을 에피소드
    -> 솔직히 잘 모르겠는데 좀 더 읽어봅시다

    152p
    요약: 강화 학습을 언어 모델에 연결하는 고리에 관한 설명들이다

    언어 모델의 인과적 토큰 예측 생성 방식을 강화학습의 관점으로 보면
    토큰 생성 = 하나의 행동이다(그럼 전체 시퀀스 생성은 에피소드, LLM은 에이전트???)
    언어 모델은 행동을 취할 때마다 보상을 받지는 않고
    전체 생성 결과에 따라 리워드 모델의 점수를 받는다고 한다
    요약: 강화학습을 적용하여 채점할 경우 생성된 문장 전체에 점수를 매긴다

    요약: 채점 모델을 사용한 강화학습에서 생기는 문제점에 관한 설명들

    보상 해킹: 보상을 높게 받는데에만 집중해서, 원하는 동작이 안나오는 경우
    코테 예시에서는 가독성 좋은 코드가 정작 코드 효율, 기능 구현 점수는 구리게 나오는 경우로 설명한다
    예를 들어 코드를 아예 안쓰거나 print('hello world')같이 간단한 점수만 쓰는 경우
    이에 대한 해결책으로 코드를 '일부만' 수정해서 가독성을 높이는 방법 즉 함수 묶기, 변수명 변경 등을
    하나씩 시도하여 가독성을 높이는 시도를 하도록 함. 이는 강화 학습 방식 중 '근접 정책 최적화'에 대응하는 모양이다

    근접 정책 최적화(Proximal Preference Optimization): 강화 학습 알고리즘으로, 기존 정책과 새로운 정책 간의 차이를 제한하여 안정적인 학습을 도모한다.

    지도 미세 조정 모델의 기준에서 멀리 떨어지지 않은 곳에 학습 모델을 두는 방법
    근데 '거리'라는게 정확히 어떤 개념인지는 설명안해줬다???
    나중할거: PPO도 구현해 봐야겠지?

    RLHF: 인간 피드백 기반 강화 학습
    쓰려면 리워드 모델을 학습 시켜야하는데, 일관성 있는(robust), 성능 좋은 리워드 모델이 필요하다
    모델 학습에 RLHF를 쓰려면 참고 모델(=지도 미세 조정 모델), 학습 모델, 리워드 모델 3개가 필요해 자원 많이 들어감

    PPO랑 RLFH를 같이 썼던 모양이다
    지금도 어느 대답이 더 나으세요? 하고 물어보긴 하더라
}