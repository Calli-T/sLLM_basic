179p
그레이디언트 누적: 배치마다 모델을 업데이트하지 않고,
여러 배치의 학습 데이터를 연산한 다음 모델을 업데이트하여
더 큰 배치 크기를 사용하는 것 같은 효과를 내는 방법

그레이디언트 체크포인팅: 순전파의 결과를 모두 저장하지 않고 일부만 저장해
학습 중 GPU 메모리의 사용량을 줄이는 방법

위 두 방법은 모델 학습시에 배치 크기를 키워
모델의 학습을 더 빠르고 안정적으로 만들어 준다.

{   그레이디언트 누적
    180p
    그레이디언트 누적의 코드의 세부사항
    학습 인자의 gradient_accumulation_steps가 몇 회 누적인지 정하는 겂이다
    해당 값의 회수만큼 누적 -> 손실을 해당 값 만큼 나누어 역전파 수행 -> 모델을 업데이트
    배치가 N배로 커진 것과 동일한 효과
    인자는 허깅페이스 transformers lib에 학습 매개변수로 전달가능

    사용례
    16배치에서 OOM 에러가 발생한다면, 배치 크기를 4, 그레이디언트 누적을 4로 하면 되는 모습을 보여준다

    장단
    그레이디언트 누적을 사용하면 더 큰 배치크기와 같은 효과를 얻을 수 있으나
    추가적인 순전파 및 역전파 연산을 수행하기 때문에 학습 시간이 증가된다.
    (작은 배치마다 그레이디언트를 계산하고, 이를 누적하기 때문이다)
}

{   그레이디언트 체크포인팅
    그레이디언트 체크포인팅의 코드와 세부사항
    그레이디언트의 역전파 계산을 위해서는 순전파의 결과를 모두 저장해야하는데,
    여기서 메모리 사용을 줄이는 방법이 있다

    방식
    사용이 끝난 데이터를 삭제하는 방법과,
    필요한 최소 데이터만 저장하고 나머지는 필요할 때 다시 계산하는 방식,
    이를 절충한 방법이 그레이디언트 체크 포인팅이다

    값을 중간 중간 저장(= 체크포인트)하여
    체크포인트부터 다시 계산해 순전파 계산량을 줄이는 것이다.

    예시
    183p의 예제에서는 1행 1,3,5열의 노드를 체크포인트로 잡고,
    4열의 노드에서 역전파 계산을 위해 네 번째 노드의 값이 필요해지면
    세 번째 노드에서부터 순전파를 계산하여 필요한 값을 주는 방식이다

    장단
    메모리 사용량은 줄지만
    당연히, 순전파를 여러번 하므로 학습 시간이 늘어나는 단점이 있다

    코드
    184p
    사용은 다른데서 선언한 함수들로 이루어져있고
    학습 매개변수는 transformers lib의 코드로 전달된다
    (pytorch가 아님)
}