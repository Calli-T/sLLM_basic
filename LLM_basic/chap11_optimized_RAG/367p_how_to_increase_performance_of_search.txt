366p
이번 장의 내용
1. 교차 인코더로 필터링된 소수의 문장을 더 정확하게 유사도를 계산하는 법(= 순위 재정렬)
2. 사전 학습된 임베딩 모델을, 사용하려는 데이터에 맞춰 미세조정해서 검색 성능을 향상
3. 바이 인코더와 크로스 인코더를 결합해서 검색 성능을 향상

요약: 미세 조정과 교차 인코더 사용법, 교차와 바이 인코더의 결합을 모두 사용한 RAG 성능 향상

367p
{   검색 성능을 높이는 2가지 방법
    요약1: 바이 인코더로 임베딩 검색을 하고, 상위 k개에 대해서 교차 인코더를 사용하면,
    교차인코더의 부담이 줄어들면서도 정확한 검색을 할 수 있다.
    요약2: 바이 인코더를 입력으로 들어올 것과 유사한 데이터를 추가 학습해서 미세조정

    RAG에 무한정 넣지않고, 상위 N개만 넣기 때문에 검색 성능을 높여야 잘 동작한다
}

368p
{   언어 모델을 임베딩 모델로 만드는 방법
    {   아키텍쳐
        사전학습된 언어 모델 (※ 보통 encoder-only)의 값을 풀링하면 끝
        풀링은 CLS토큰, 평균, 최대가 있는데 보통 평균을 많이 쓴다
    }

    {   학습방법: 대조학습
        문장 임베딩을
        - 관련이 있거나 유사한 데이터는 가깝게, 관련이 없거나 유사하지 않은 데이터는 더 멀게
        - 이어지는 문장이면 서로 가깝게, 아니면 서로 멀게,
        - 서로 Q&A 문장이면 가깝게, 아니면 서로 멀게
        뭐 어쨌거나 유사 문장은 가깝게, 무관 문장은 멀게 하면된다

        데이터셋은?
        - 실습에서는 2개의 문장이 유사한 정도를 점수로 매긴 KLUE STS 데이터셋 사용
        - 위 내용으로 학습한 모델을 바탕으로, Q&A 문장이 서로 가깝게 나오도록 미세 조정함, 이 때는 MRC데이터셋으로 학습함
        - STS-B는 '사람이' 점수를 매겼음
        - Positive / Negative Pair등으로 직접 데이터셋을 만들 수 있는데, QNA데이터셋같은 경우 고객 질문 - 실제 답변은 positive로, 무관한 다른 고객 답변은 negative로, 애매한건 중간 점수로 처리하는 방법이 있음

        그래서 학습을 어떻게 할건데?
        - 두 문장의 임베딩을 코사인 유사도 구함 - 이를 데이터셋의 점수를 [0, 1]로 정규화 해서 오차를 구하고, 이 오차를 역전파

        요약: 언어모델을 유사 문장으로 학습 -> 후 Q&A 데이터로 미세 조정 -> 검색 쿼리와 관련 있다면, 유사도가 높게 나온다

        실습용 코드는 370p부터
    }
}

370p
어쩌다 알게된 사실
llama-3.1을 fine-tuning한 bllossom조차도, tokenizer.json 영어단어 밖에 없다
한국어도 토큰이 있긴한데, 깨져있다
tokens = tokenizer.tokenize("한국어를 사용합니다.")
Tokenized: ['íķľêµŃ', 'ìĸ´', 'ë¥¼', 'ĠìĤ¬ìļ©', 'íķ©ëĭĪëĭ¤', '.']
Token IDs: [112699, 32179, 18918, 41820, 61938, 13]
이런 식으로 나옴
tokens = tokenizer.tokenize("한국어를 적용합니다.")
Tokenized: ['íķľêµŃ', 'ìĸ´', 'ë¥¼', 'Ġìłģìļ©', 'íķ©ëĭĪëĭ¤', '.']
Token IDs: [112699, 32179, 18918, 115839, 61938, 13]

tokens = tokenizer.tokenize("한국어 사용합니다.")
Tokenized: ['íķľêµŃ', 'ìĸ´', 'ĠìĤ¬ìļ©', 'íķ©ëĭĪëĭ¤', '.']
Token IDs: [112699, 32179, 41820, 61938, 13]

이런거 보면 깨진게 문제지 한국어 자체는 토크나이저 안에 있긴함