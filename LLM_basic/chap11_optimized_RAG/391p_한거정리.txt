1. 임베딩 모델 제작: klue(한국어 이해 평가 데이터셋)으로 사전학습된 roberta small모델에 (주로 mean) pooling 달아서 embedding model 만들기
2. 임베딩 모델 사전학습: 그걸 sts데이터로 사전학습시키기
3. 미세조정: 그걸 mrc데이터로 question과 context(answer가 포함된) 미세조정하기, 이 경우 손실 함수는 MNR이며 배치 내부의 원본 idx와 다른 context를 오답으로 negative-pair 처리한다
4. cross encoder 제작: klue/roberta small 모델을 하나 더 만들어서, 교차인코더로 활용, 이 경우 CrossEncoder class를 쓰면 자동으로 분류기를 끝에 달아준다. encoder-only 모델 아무거나사용가능
5. cross encoder 미세조정: 이를 mrc 데이터셋으로 미세조정, 질의과 정답(이 포함된 문맥, 혹은 원문)을 긍정-쌍/부정-쌍을 만들어 학습, 라벨링은 긍정 1 부정 0
6. 순위 재정렬: 교차인코더를 사용해서 임베딩 서치 결과 상위 k를 대상으로 순위 재정렬
7. 성능 평가: 교차 인코더는 상위 3k개를 뽑아서 재정렬한 다음 k개를 뽑음