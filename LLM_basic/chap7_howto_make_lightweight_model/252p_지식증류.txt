지식 증류: 더 크고 성능이 높은 선생 모델의 생성 결과를 활용해, 더 작고 성능이 낮은 학생 모델을 만드는 방법
학습 방식: 학생 모델은 선생 모델의 생성 결과를 모방함
증류란: 크기가 더 작고 선생 모델의 지식을 더 작은 모델로 압축함으로 증류라 부름

요샌 선생 모델로 대규모 학습 데이터셋을 구축하거나,
사람이 선별할 것을 LLM이 선별하는 등 사용 가능

sLLM의 구축에는 LLM을 사용하는 것이 가능,

사례로 제퍼와 파이가 있다

{   제퍼의 경우
    지시 데이터셋의 구축과,
    선호 데이터셋의 구축에 모두 LLM을 사용
    (GPT의 경우 레이블러를 고용하고, 프롬프트에 대한 응답을 직접 작성함)
}

{   파이의 경우
    파이썬 코딩을 배우는데 필요한 학습 데이터를,
    GPT-3.5를 사용함

    중요한 로직이 있는지 없는지,
    쓸모 없는 코드는 아닌지, 혹은 설정용 코드인지 등등
    코드를 선별하는 작업을 GPT-3.5로 선별함
}

두 경우 개발이 모두 빨라졌고, 성능도 좋다

7장 요약:
KV캐시 줄여서 배치 많이 올리기, 양자화 해서 모델 크기 줄여서 배치 많이 올리기, 지식 증류로 성능 늘리기
-> 이건 모두 성능 저하를 일부 감수하면서 효율적인 추론 하는 법임

성능 저하 없이 추론을 효율적으로 하는 법