일반 GPT는 단순히 다음 단어를 예측하는 방식으로 학습
채찍피티는 여기에 Q&A 지시 데이터셋과,
선호하는 답변을 선택하여 학습하는 방식

선호를 학습하는 방법은
근접 정책 최적화(PPO)로 대표되는 강화학습 방식과
강화 학습을 사용하지 않고 선호를 학습시키는 방식이 있다고 한다

RLHF와 PPO는 강화학습 방식
기각 샘플링과 직접 선호 최적화는 강화학습 방식이 아님

137p
사전학습 동안에는 여러 글을 에서 다음 단어를 예측하는 방식으로 학습
이 기간에는 요청에 응답하는걸 기대하긴 어려움

138p
지도 미세 조정에 관한 설명
사전 학습하고난 LLM을 '정답이 포함된' 학습 데이터로 학습하는 것
지시 데이터셋

채찍피디는 데이터 레이블러를 고용해 13000개의 지시 데이터셋을 구축했다고 한다

141p
지시 데이터셋의 형태
사용자의 요구 + 입력 + 출력을
모두 때려넣은 텍스트를 LLM에 집어 넣었고, 저 셋 끼리의 구분은 ###으로 해놨다
저게 한 string으로 된 이유가 아마 디코더 위주 모델이기 때문인듯
그거 템플릿 아마 파이썬? 식으로 만들어서 입력으로 만들어 둔게 142p
결론적으로 학습 방식 자체는 인과적 언어 모델링인 사전 학습이랑 똑같다

요약
사전학습 -> 지도 미세조정, 방식은 같고 데이터셋이 다름

142p~
{   다음 요약으로 넘어가쇼
    지시 데이터셋의 정답은 아직 없다고함
    좋은 지시데이터셋을 얻는 과정을 설명함

    서양식 네이버 지식인 위키하우나 스택익스체인지에서 가져옴
    지시사항이 다양하고 품질이 좋을수록 정렬한 답변 품질이 좋다고함

    피상적인 정렬 가설: 적은 데이터셋으로 정렬이 가능한 이유를 설명한 가설
    모델의 지식이나 능력은 사전 학습에서 대부분 학습하고 정렬 데이터를 통해서는 형식이나 템플릿정도만 배우는거라 적은 데이터셋으로 가능하다고 (메타에서) 주장함

    144p부터는
    지시 데이터셋의 품질에 관한 얘기
    데이터를 선별해서 더 나은 품질을 가졌다는 얘기이다

    파이썬 긴 주석에 함수가 하는 일을 잘 정의한, 풀어 써둔 코드를 더 잘 이해하는듯하다
    +

    코드 예제 데이터셋을 생성해 학습 데이터를 구축했다고 함

    요약: 데이터 품질이 좋으면 언어 모델으 성능이
    긴 주석을 잘쓴 코드 (코딩 답변을 학습하는데) 품질이 좋은 데이터셋으로 취급한다고 함
    그리고 예제 문제 개념으로 예제코드를 추가하는것도 좋다고함
    + 함수의 이름에 기능이 담긴, 흔히 말하는 '잘쓴 코드'가 지시 데이터셋으로써의 품질이 좋다고 함

}
147p
좋은 지시 데이터셋이 갖춰야할 조건 요약
-다양한 지시사항의 형태
-고품질 답변
-교육적 가치가 낮은 데이터를 필터링
-예제 데이터를 추가
-이 조건을 갖춘다면 작은 규모라도 좋은 가능함