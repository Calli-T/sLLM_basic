{   LLaVA
    LLaVA: 간단히 말해서, 이미지와 텍스트를 동시에 물어 볼 수 있는 모델이다.
    이미지 토큰 시퀀스랑 텍스트 토큰 시퀀스를 서로 concat해서 대답해주는 모델인듯 하다

    GPT-4o나 GPT-4V같은 더 좋은 모델이 있긴한데, 일단 얘는 오픈소스고, 더 좋은게 있나 찾아나보자

    {   LLaVA의 학습 데이터
        ChatGPT와 GPT-4로 데이터셋을 생성했다
        GPT-4는 텍스트 모델이기 때문에, 이미지 자체는 줄 수 없으나

        이미지에 대한 설명 / 박스(오브젝트 각각에 하나 당 네 귀퉁이의 바운더리 좌표)
        이렇게 3가지 정보를 주어 이미지를 인식하게 하고,

        GPT-4로 하여금 이미지 설명을 보고
        1) 질문 했을 때 대화 ※ 이미지 안의 물체, 물체의 수 등등
        2) 자세한 이미지 설명 ※ 이미지 설명을 읽고
        3) 복잡한 추론
        이 3가지를 생성하도록 했다.

        요약: GPT-4에 캡션과 바운더리 박스를 주고 / 질답, 이미지 설명, 추론 텍스트를 생성시켰다
    }

    {   LLaVA의 모델 구조
        CLIP(모달리티 인코더)과 LLM 백본 사이 있어야할 입력 프로젝트를 linear layer로 구현함
        이후 이미지 시퀀스는 패치 각각이 토큰이 되어 linear layer를 통과해서 토큰 임베딩이 되고,
        텍스트는 LLM의 처리 방식 그대로 토큰 임베딩이 된다
        이 둘을 concat하여 LLM에 넣었음
    }

    {   LLaVA 1.5의 업뎃 사항
        ViT를 더 좋은걸 쓰고, linear 하나 쓰던걸 MLP로 바꿨다
    }

    {   LLaVa NeXT의 업뎃 사항
        입력 해상도 4배
        고품질 지시 데이터셋 구축
        더 많은 시나리오(이건 지도 미세 조정에 쓴건가 ?)
        SGLang 프레임워크 사용해서 추론 성능 향상 ※ 이건 뭔소린지
        ※ 얜 GPT-4V랑 비교해도성능이 80~90%는 나오는 모양이다
    }

    뭐... 1.5나 NeXT 보다 좋은 오픈소스는 늘 나왔을테니, SFT 할 때 다시 찾아보자
}